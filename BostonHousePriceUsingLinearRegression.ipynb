# Boston House Dataset -> Linear Regression problem
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as snb

# Load the dataset into data
data = load_boston()
#print(data.data)
#print(data.target)
X = data.data
y = data.target

# EDA by correlation analysis and Feature Selection
df = pd.DataFrame(X, columns = data.feature_names)
#df.head()
#plt.figure()
#snb.heatmap(df.corr(),annot=True,linewidths=0.1, square= True,fmt='.2g',center=False)
#plt.show()
# Features with correlation both positive and negative , greater than 0.5 are considered to be good for prediction model.
# From heatmap these features with greater than 0.7 correlation are ['INDUS','NOX','AGE','DIS','RAD','TAX','ZN','CRIM']
# Corr graph also shows that number of room 'RM' does not have higher coefficient and thus, would not effect the prediction 
# model to high. 
#plt.figure()
#snb.heatmap(df[['INDUS','NOX','AGE','DIS','RAD','TAX','ZN','CRIM']].corr(),annot=True,linewidths=0.1, square= True,fmt='.2g',center=False)
#plt.show()
# Re-frame data 
X = df[['INDUS','NOX','AGE','DIS','RAD','TAX','ZN','CRIM']] 
# Instantiate LinearRegression module
X_train, X_test, y_train, y_test = train_test_split(X,data.target,test_size = 0.3, random_state = 42)

lr = LinearRegression()
lr.fit(X_train,y_train)
lr.predict(X_test)

# Model Evaluation - Residual Analysis, Mean_Square_errors, R2_score
# Residual Analysis - Overlapping of y_test and y_pred
y_train_pred = lr.predict(X_train)
y_test_pred  = lr.predict(X_test)

plt.figure(figsize=(12,8))
plt.scatter(y_train_pred, y_train_pred-y_train, c='blue', marker='*',label='Training Data')
plt.scatter(y_test_pred, y_test_pred-y_test, c='red', marker='o',label='Testing Data')
plt.xlabel('Predicted Values')
plt.ylabel('Residual Values')
plt.legend()
plt.hlines(y=0,xmin=-40,xmax=40, lw=0, color = 'k') # setting to same scale
plt.xlim([-5,40])
plt.ylim([-35,15])
plt.show()
# MSE
print("Mean Square error for testing data:", mean_squared_error(y_test,y_test_pred))    #35.1634712907
print("Mean Square error for training data:", mean_squared_error(y_train,y_train_pred)) #55.5871406104
print("Coefficient of Determination for testing data:", r2_score(y_test,y_test_pred))   #0.528090048834
print("Coefficient of Determination for training data:", r2_score(y_train,y_train_pred))#0.367584351951

#Note: Looking t the values for testing and training data, there are huge differences between actual data and predicted data. 
#      The differences could be due to either, unfit feature selection or, presence of outliers.
#      We need to handle outliers or consider robust regression model as it is resistive to outliers.



